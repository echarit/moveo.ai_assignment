[**Moveo.AI**](http://Moveo.AI) **Assignment \- Charitos Ioannis**

1) **Introduction**

The topic of this report is the evaluation of a RAG system through a sample dataset of 25 user queries paired with the system’s responses. The evaluator performs the task automatically without any human intervention and utilizes metrics both from classical information retrieval domain as well as the more contemporary field of Generative AI in particular LLMs (Large Language Models).

2) **Dataset and Metric Definitions**

The dataset consists of .csv files of 25 total examples (rows) from different topics each having a total of 4 features (columns):

* **Current User Question**: The latest query of the user for each case  
* **Conversation History**: The previous session the user might have had with the agent.  
* **Fragment Texts**: The top-k (assuming) relevant text passages that were retrieved from the RAG system  
* **Assistant Answer**: The final answer the RAG system generated for the user.

As stated in the introduction the automatic evaluation script (also referred as judge) will combine both techniques from classical information retrieval as well as metric defined by its designer which might or might not map to already established techniques/metrics for evaluating AI generated linguistic content such as faithfulness. It should be stated however that in the evaluated csv file that is generated by the evaluator, each evaluation dimension will have a numerical value assigned to it whenever it is possible. The reason behind this rationale is two-fold. First, it is to reduce the evaluation of the judge itself into an engineer task rather than a linguistic tax as much as possible. Having entirely textual evaluation dimensions might have its merits but it makes the final assessment open to such a large degree of subjectivity that is distractive and not that productive especially in the absence of domain experts that might help counter that. Second, having numerical values as evaluation dimensions helps bridging the gap between the retrieval and generative aspect of a RAG system thus making its evaluation procedure much more consistent.

3) **Evaluation dimensions**

 A RAG system (Retrieval Augmented Generation) as its name suggests is a system that combines both Retrieval and Generative techniques to generate the final result i.e. a response for a user. Therefore it is fairly logical to start the evaluation with a set of metrics appropriate for the retrieval part. This set consists of two metrics: Average Retrieved Similarity and  LLM Precision@k.

**3.1) Average Retrieved Similarity**

A typical RAG system works by retrieving a set of N (ordered) relevant documents by using a vector similarity metric between a vectorized version of the user’s query and a set or vector database of vectorized documents. Such a metric might be the cosine similarity inside said vector space. Ideally the process/model that produced the vectorized versions of both ends shall be the same. In any case this process can be easily reversed engineered in order to evaluate the quality of the retrieval part of our RAG system. In particular to generate the Average Retrieved similarity for a given data point (user query) the following steps are followed:

* **Pick a vectorization model**:  For this task the "qwen14:b" and "qwen8:b" LLMs  model from the *sentence\_transormers* library was chosen as it is one of the best model for producing sentence/document embeddings which in combination with its small size make it an ideal candidate for this step. More complex models may also be used for this task.  
* **Vectorization**: Using "all-MiniLM-L6-v2" , vectorize both the Current User Query concatenated with the Conversation History column and each element from the  Fragment texts column which are separated by the newline (‘\\n) character.   
* **Calculate Similarities**: For each user query calculate N similarities where N is the number of fragment texts respective to that user query.  
* **Average Similarity**: Compute the mean of N similarities from the previous step. The final result is this row’s Average Retrieved Similarity metric.


  
From this definition it stems that the Average Retrieved Similarity is a metric in the range \[-1, 1\]. Similar text passages would have a vector similarity of 1 and totally dissimilar texts 0 respectively. In the following table we can see the Average Similarity Metric for the first 5 user queries by order of appearance in the sample dataset:

| Current User Question | Average Retrieved Similarity |
| :---- | :---- |
| What are the benefits of using renewable energy? | 0.909 |
| How does photosynthesis work? | 0.906 |
| Can you explain what a black hole is? | 0.871 |
| What does GDP measure? | 0.873 |
| What is the Pythagorean theorem? | 0.871 |

Table 1:  Average Fragment Cosine Similarity of the 5 user questions in the sample dataset  
To testify the vectorization quality of the model in the Appendix the average vector similarities of these 5 questions along with 5 random sentences are presented.

**3.2) LLM Precision@k**

The next metric is called LLM precision@k and it is a variant of the classical precision@k metric that is used to evaluate information retrieval methods. Its definition is the same as the one used in information retrieval with a simple twist: Since there are no dataset labels which denote which retrieved document is relevant and which is not, an LLM will be leveraged to classify a document as relevant or not with the use of a simple prompt. The prompt for this metric can be found in the file [constants.py](http://constants.py) in the delivered project under the name LLM\_PRECISION\_AT\_K\_PROMPT. The process to evaluate a fragment text as ‘relevant’ or not goes as follows:

* **Pick an  evaluator model:**  For this task the "qwen14:b" and "qwen8:b" LLMs  will serve as the evaluator of a fragment text with respect to the Current User Question column.  
* **Structured output**: The model is instructed through its prompt to generate a strictly formatted json file. For each fragment text two fields are returned from the LLM. One named “answer” which is binary valued YES or NO and another one named ‘explanation’ which provides feedback regarding the LLM’s decision. These json files are stored in the *data/{model\_name/llm\_precision\_at\_metrics.json* directory for logging, debugging and transparency reasons.  
* **Aggregate Relevant documents:** For each fragment text classified as ‘relevant’ by the LLM w.r.t. to the user question, raise the “True Positive” number by one.Then LLM precision@k metric can be calculated as an image of its traditional counterpart. This number is stored in an auxiliary column named True Positives which will be later used to calculate an aggregate version of this LLM precision@k metric in a micro average matter.

The range of this metric is \[0-1\] just like its classical version. Thus the higher it is the more relevant the retrieved fragments are. Combined with Average Retrieved Similarity these metrics can quantify the retrieval part of the RAG system in an intuitive and easy to interpret way.  

Note: For this use case the fragmented texts Possible candidate metrics adjacent to this that were considered were recall@k and  hits@k. The former was discarded as it needs a set of texts that were not retrieved by the system. Such information is not only unavailable but really difficult to measure in general as it cannot be really known a priori what a RAG system should have retrieved in the first place. The latter was discarded as it is a very simple and “generous” metric especially taking into consideration the definition of relevance used here.

**3.3) Verbosity**

The next metric is called Verbosity and it is a fairly simple one. Given the absence of user instructions in the Current User Question column as well as in the assignment’s instructions whatsoever it is fairly logical to assume that a RAG answer should be short and to the point without lengthy responses. The verbosity metric aims to measure exactly that again with the use of an LLM and the use of a simple prompt. The prompt for this metric can be found in the file [constants.py](http://constants.py) in the delivered project under the name VERBOSITY\_PROMPT. The process is fairly similar with the LLM precision@ k metric in terms of its design. In particular:

* **Pick an evaluator model:**  For this task the "qwen14:b" and "qwen8:b" LLMs  will serve as the evaluator of the Assistant Answer column w.r.t to the Current User Question one.  
* **Structured output**: The model is instructed through its prompt to generate a strictly formatted json file. For each assistant answer text two fields are returned from the LLM. One named “score” ranging from 1-5 depending on the verbosity of the answer with 1 for lowest and 5 for highest verbosity. The final score is then inverted i.e. 1/score and stored in the Verbiosity column in the evaluated\_dataset.csv file. in order to reflect an actual verbosity score in the aggregate statistics calculation step. The aforementioned json files are stored in the *data/{model\_name/verbosity\_metrics.json* directory for logging, debugging and transparency reasons.

**3.4) Alignment**

The next metric is called Alignment. This metric tests the ability of the RAG system to respond according to humanitarian values rather than strictly to the user’s content. For example if the user asks for a way to cause potential harm the system should deny that request. If a RAG system fails to behave accordingly then it should be negatively evaluated. The calculation of the alignment metric is a two step process. The first one is to determine the user’s intent through the question he or she submitted to the system. For simplicity the user intent will be a binary metric: **Malignant** or **Benign**. In order to classify the user’s intent, the automatic evaluator will be using a distinct prompt found in the file [constants.py](http://constants.py) in the delivered project under the name USER\_INTENT\_PROMPT. Then an auxiliary column will be generated with the name “User Intent” holding the LLM’s assessment.   
The LLM once again is instructed to produce structure output for each user question with two fields: ‘intent’ which has the Malignant or Benign value and another one called “explanation”. These json files are stored in the *data/{model\_name/user\_intent\_metrics.json* directory for logging, debugging and transparency reasons.  
The second step is to calculate the actual Alignment metric with respect to the User Intent Column. The alignment itself is a binary value: **True** or **False.** The truth table of this metric goes as follows:

| Alignment Metric  Value | User Intent | RAG response |
| :---- | :---- | :---- |
| True | Benign | Complying |
| True | Malignant | Refusing |
| False | Benign | Refusing |
| False | Malignant | Complying |

Table 2:  The way the Alignment metric values are calculated by the automatic evaluator.

Note: The Alignment metric is not concerned with the truth value of the RAG’s response (faithfulness) as this is the evaluation dimension of the next and final metric. It is concerned only with Alignment as it is defined in Table 2\.   
Given all that the process to calculate the Alignment metric goes like this:

* **Pick an evaluator model:** For this task the "qwen14:b" and "qwen8:b" LLMs will serve as the evaluator of the for both the Auxiliary User Intent column as well as the Alignment. After the User Intent is calculated for a given User query, that query along with its user intent value  and the Assistant Answer will be fed into the LLM. The prompt for this step can be found in the file [constants.py](http://constants.py) in the delivered project under the name ALIGNMENT\_PROMPT.   
* **Structured output**: The model is instructed through its prompt to generate a strictly formatted json file. For each assistant answer text two fields are returned from the LLM. One named “alignment” with the binary values True or False according to table 2\. The final score is then converted to numerals (0, 1\) and stored in the Alignment column in the evaluated\_dataset.csv file.  The aforementioned json files are stored in the *data/{model\_name/alignment\_metrics.json* directory for logging, debugging and transparency reasons.

Therefore the sum of the column for all 25 user queries reflects the total system’s ability to produce appropriate answers.

**3.5) Faithfulness**

The next and final score is the established metric of faithfulness. As its name suggests this metric reflects the degree to which the RAG’s response is adjacent to objective reality. It could also be defined as the unit measure of the system’s hallucination. For simplicity the faithfulness metric will take values in the range of \[1-5\] where 1 means a very factually wrong or hallucinative RAG response while 5 means a very concise answer respectively. In order to classify the Faithfulness of an Assistant’s Answer w.r.t to a User Query, the automatic evaluator will be using a distinct prompt found in the file [constants.py](http://constants.py) in the delivered project under the name FAITHFULNESS\_PROMPT. Given all that the process to calculate the Alignment metric goes like this:

* **Pick an evaluator model:**  For this task the "qwen14:b" and "qwen8:b" LLMs  will serve as the evaluator of the Assistant Answer column w.r.t to the Current User Question one.  
* **Structured output**: The model is instructed through its prompt to generate a strictly formatted json file. For each assistant answer text two fields are returned from the LLM. One named “faithfulness” with the values in the range {0-5} and another one called “explanation” where the model’s justification for its scoring is stored. The aforementioned json files are stored in the *data/{model\_name/faithfulness\_metrics.json* directory for logging, debugging and transparency reasons.

4) **Aggregate statistics**  
     
   Having defined the evaluation dimensions in such a manner a general picture of an LLM’s and thus the automatic evaluator’s can be obtained by simply calculating the mean values for all 25 user queries with the exception of precision@k where a micro averaging across all True Positives is performed. The following tables show the aggregate statistics for the two Qwen models with 14 and 8 billion parameters respectively which were tested:  
     
   

| Model: Qwen14:b |  |
| ----- | ----- |
| Aggregagate Metric | Value |
| Average Retrieved Similarity | 0.769 |
| Precision@k | 0.979 |
| Verbosity | 0.44 |
| Alignment | 0.72 |
| Faithfulness | 0.728 |

Table 3:  Aggregate statistics of the Qwen 14:b model..

| Model: Qwen8:b |  |
| ----- | ----- |
| Aggregate Metric | Value |
| Average Retrieved Similarity | 0.769 |
| Precision@k | 0.966 |
| Verbosity | 0.55 |
| Alignment | 0.92 |
| Faithfulness | 0.632 |

Table 4:  Aggregate statistics of the Qwen 8:b model.

At first glance the 8 billion model seems to be performing better in terms of aggregate metrics which is counterintuitive given the heuristic that the more parameters an LLM has the better its general performance is. This is addressed in the error analysis where with the help of a human evaluator the shortcomings of the 8 billion model become evident.

NOTE: A total of three more models named "qwen4:b", "qwen1.7:b" and "qwen0.6:b" were also tested but their inability to produce consistently structured output as instructed in their prompts lead to their omission for the current report.

5) **Error Analysis**

6) **Appendix**

 The following tables show the similarity between the first 3 questions in order of appearance in the sample dataset of 25 User Questions. As it can be seen the model’s vector space is very keen on separating semantically different sentences given their low cosine similarities with the first 3 questions. That makes it an ideal model for the Average Retrieved Similarity metric

| Current User Question: What are the benefits of using renewable energy? |  | Vector Cosine Similarities with   "all-MiniLM-L6-v2" |
| ----- | ----- | :---- |
| Sentence |  |  |
| The cat sleeps in the car. |  | \-0.024 |
| An apple a day keeps the doctor away. |  | \-0.016 |
| Good things come to those who wait. |  | 0.051 |
| One plus one equals two. |  | 0.015 |
| Press any button to continue. |  | 0.053 |

Table 5: Vector cosine similarity between the first user question in order of appearance in the provided for evaluation dataset..

| Current User Question: How does photosynthesis work? |  | Vector Cosine Similarities with   "all-MiniLM-L6-v2" |
| ----- | ----- | :---- |
| Sentence |  |  |
| The cat sleeps in the car. |  | 0.044 |
| An apple a day keeps the doctor away. |  | 0.075 |
| Good things come to those who wait. |  | 0.026 |
| One plus one equals two. |  | 0.081 |
| Press any button to continue. |  | 0.129 |

Table 6: Vector cosine similarity between the second user question in order of appearance in the provided for evaluation dataset.

| Current User Question: Can you explain what a black hole is? |  | Vector Cosine Similarities with   "all-MiniLM-L6-v2" |
| ----- | ----- | :---- |
| Sentence |  |  |
| The cat sleeps in the car. |  | 0.072 |
| An apple a day keeps the doctor away. |  | 0.135 |
| Good things come to those who wait. |  | 0.020 |
| One plus one equals two. |  | 0.139 |
| Press any button to continue. |  | 0.084 |

Table 7: Vector cosine similarity between the second user question in order of appearance in the provided for evaluation dataset.

